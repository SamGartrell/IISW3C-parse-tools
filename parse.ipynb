{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing IICW3C logs\n",
    "python environment, functions, and examples\n",
    "<br>\n",
    "7/27/23\n",
    "<br>\n",
    "\n",
    "## environment\n",
    "- use conda to clone the environment stored in `environment.yml`\n",
    "    ```\n",
    "    conda env create -f environment.yml\n",
    "    conda activate iisw3c-tool\n",
    "    ```\n",
    "\n",
    "- if using this notebook, set the kernel to 'parse-iis'\n",
    "\n",
    "- you will also need a `.env` file with credentials for the `geoip2` geocoder, represented as a key stored under `GEOAPI2_KEY` and an account id stored under `GEOIP2_AID`. you can set up a free account [here](maxmind.com/en/geoip2-databases). the `.env` should also contain `LOG_LOCATION`, representing a file path where logs (.log files) are stored in folders for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pprint\n",
    "import time\n",
    "import pandas as pd\n",
    "warnings.simplefilter(\n",
    "    action='ignore', category=pd.errors.SettingWithCopyWarning\n",
    "    )\n",
    "from geoip2 import webservice, errors\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import statistics\n",
    "from user_agents import parse\n",
    "\n",
    "if load_dotenv():\n",
    "    print('env variables loaded')\n",
    "\n",
    "logfile_location = os.getenv('LOG_LOCATION')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "Use these functions to access and analyze data stored in local log files. Recommend first developing analysis on a single log, then expanding it to target the month or set of months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_df(\n",
    "        log_path:str,\n",
    "        fields:list=['date', 'time', 's-ip', 'cs-method', 'cs-uri-stem', 'cs-uri-query', 's-port', 'cs-username', 'c-ip', 'cs(User-Agent)', 'cs(Referer)', 'sc-status', 'sc-substatus', 'sc-win32-status', 'sc-bytes', 'time-taken']\n",
    "        ):\n",
    "    \"\"\"\n",
    "    loads a log file into a pandas dataframe\n",
    "    \"\"\"\n",
    "    # Read the log file into a DataFrame\n",
    "    df = pd.read_csv(log_path, sep=' ', header=None, names=fields, skiprows=4)\n",
    "\n",
    "    # add a unique ID to dataframe so it can be cut into subsets\n",
    "    df['uid'] = range(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def logs_to_df(\n",
    "        folder_path:str,\n",
    "        fields:list=['date', 'time', 's-ip', 'cs-method', 'cs-uri-stem', 'cs-uri-query', 's-port', 'cs-username', 'c-ip', 'cs(User-Agent)', 'cs(Referer)', 'sc-status', 'sc-substatus', 'sc-win32-status', 'sc-bytes', 'time-taken'],\n",
    "        filter_mm_rlis=False):\n",
    "    \"\"\"\n",
    "    loads a folder of log files into a single pandas dataframe. all log files must have the same fields, and follow the ISSW3C spec.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # initialize list for per-log dataframes, later flattened\n",
    "    dfs = []\n",
    "\n",
    "    # for each log file in the target folder,\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.log'):\n",
    "            print(f'\\t\\tloading {file} to dataframe')\n",
    "            log = os.path.join(folder_path, file)\n",
    "            \n",
    "            # read it into a dataframe (ignore 1st 4 lines, in keeping with ISSW3C format)\n",
    "            # add it to the list\n",
    "            if filter_mm_rlis:\n",
    "                df = filter_metromaps_rlis(\n",
    "                    pd.read_csv(log, sep=' ', header=None, names=fields, skiprows=4)\n",
    "                )\n",
    "            else:\n",
    "                df = pd.read_csv(log, sep=' ', header=None, names=fields, skiprows=4)\n",
    "\n",
    "            dfs.append(df)\n",
    "            # print(file, \"read successfully\")\n",
    "\n",
    "    # flatten the list\n",
    "    folder_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # add a unique ID to dataframe so it can be cut into subsets\n",
    "    folder_df['uid'] = range(1, len(folder_df) + 1)\n",
    "\n",
    "\n",
    "    print(f'took {time.time() - start} sec')\n",
    "    return folder_df\n",
    "\n",
    "def geocode_ip(dataframe: pd.DataFrame, sample_size: int = 100):\n",
    "    \"\"\"\n",
    "    returns a geodataframe of points representing IP addresses from the dataframe passed.\n",
    "    Works well with a df that's already grouped by IP address, to maximize the number of unique IPs sampled and geocoded.\n",
    "    only 1000 requests/day are permitted; consider upgrading to a paid svc \n",
    "    \"\"\"\n",
    "    # geocoder api stuff\n",
    "    key = os.getenv('GEOIP2_KEY') # api key\n",
    "    aid = os.getenv('GEOIP2_AID') # acc id\n",
    "\n",
    "    # get every nth record to reduce burden on geocoder\n",
    "    subset = dataframe.sample(n=sample_size)\n",
    "    subset['lat'] = \"\"\n",
    "    subset['lon'] = \"\"\n",
    "    subset['reserved_ip'] = False\n",
    "\n",
    "    # for each row, geocode its ip\n",
    "    with webservice.Client(aid, key, host='geolite.info') as client:\n",
    "        for row_index, row in subset.iterrows():\n",
    "            try:\n",
    "                response = client.city(row['c-ip'])\n",
    "                lon = response.location.longitude\n",
    "                lat = response.location.latitude\n",
    "                subset.loc[row_index, ['lon', 'lat']] = [lon, lat]\n",
    "            except errors.AddressNotFoundError:\n",
    "                subset.loc[row_index, 'reserved_ip'] = True\n",
    "\n",
    "    spatial_subset = subset[subset['reserved_ip'] == False]\n",
    "    geo = [\n",
    "        Point(lon, lat) for\n",
    "            lon, lat in \n",
    "                zip(\n",
    "                    spatial_subset['lon'],\n",
    "                    spatial_subset['lat']\n",
    "                )]\n",
    "    \n",
    "    return gpd.GeoDataFrame(spatial_subset, geometry=geo)\n",
    "\n",
    "\n",
    "def parse_user_agent(user_agent):\n",
    "    \"\"\"\n",
    "    parses a user agent string\n",
    "    \"\"\"\n",
    "    ua = parse(user_agent)\n",
    "    \n",
    "    device = {\n",
    "        'brand': ua.device.brand,\n",
    "        'model': ua.device.model,\n",
    "        'os': f'{ua.os.family} {ua.os.version_string}'\n",
    "    }\n",
    "    \n",
    "    browser = {\n",
    "        'family': ua.browser.family,\n",
    "        'version': ua.browser.version_string\n",
    "    }\n",
    "    \n",
    "    is_bot = any(keyword in user_agent.lower() for keyword in ['bot', 'crawler', 'spider'])\n",
    "    \n",
    "    return {\n",
    "        'agent_str': user_agent,\n",
    "        'device': device,\n",
    "        'browser': browser,\n",
    "        'is_bot': is_bot\n",
    "    }\n",
    "\n",
    "\n",
    "def batch_stats(function, log_dir=fr'{logfile_location}', just_mm_rlis=False):\n",
    "    \"\"\"\n",
    "    iteratively applies a function with dict output to every month in the log folder, reporting results. expects resuts\n",
    "    \"\"\"\n",
    "\n",
    "    for i in os.listdir(log_dir):\n",
    "        print(i)\n",
    "        if i.endswith('.zip') or i.endswith('.txt'):\n",
    "            pass\n",
    "        else:\n",
    "            # make the log for the month\n",
    "            month = logs_to_df(os.path.join(log_dir, i), filter_mm_rlis=just_mm_rlis)\n",
    "\n",
    "            # get and view ip stats\n",
    "            stats = function(month)\n",
    "            print(\n",
    "                i.split('/')[-1],\n",
    "                pprint.pformat(stats)\n",
    "            )\n",
    "            month = None\n",
    "            print('__________________________________________')\n",
    "\n",
    "def ip_request_statistics(dataframe:pd.DataFrame, use_log=True):\n",
    "    \"\"\"\n",
    "    computes mean, standard deviation of request counts by IP,\n",
    "    and gives z-score of most prolific IP (base-2 logarithm-transformed if use_log==True),\n",
    "    for the dataframe passed\n",
    "    \"\"\"\n",
    "    # Calculate the request count per IP address (log-transformed)\n",
    "    request_counts = dataframe['c-ip'].value_counts()\n",
    "    if use_log:\n",
    "        request_counts = np.log2(\n",
    "            request_counts\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Calculate the mean and standard deviation of request counts\n",
    "    mean_requests = request_counts.mean()\n",
    "    std_requests = request_counts.std()\n",
    "\n",
    "    # Identify the most prolific IP address\n",
    "    most_prolific_ip = request_counts.idxmax()\n",
    "\n",
    "    # Calculate the z-score for the most prolific IP address\n",
    "    z_score = (request_counts[most_prolific_ip] - mean_requests) / std_requests\n",
    "\n",
    "    return {\n",
    "        # the mean number of requests coming from a single IP\n",
    "        'mean_request_count': mean_requests,\n",
    "\n",
    "        # the dispersion of the above\n",
    "        'stdev_request_count': std_requests,\n",
    "\n",
    "        # top 10 most prolific IP addresses\n",
    "        'top_10': list(dataframe['c-ip'].value_counts().head(10).index),\n",
    "\n",
    "        # the number of unique IP addresses\n",
    "        'n_unique': dataframe['c-ip'].nunique(),\n",
    "\n",
    "        # address, request count, and z-score (prominence) of most prolific IP in the dataframe\n",
    "        'top': {\n",
    "            'address': most_prolific_ip,\n",
    "            'request_count': request_counts.max(),\n",
    "            'z_score': z_score\n",
    "        },\n",
    "\n",
    "        # wether or not these statistics come from logarithm-transformed data\n",
    "        'used_log': use_log\n",
    "    }\n",
    "\n",
    "def filter_services(dataframe):\n",
    "    '''\n",
    "    metro-specific: get insights on /services/ endpoints that aren't from ArcGIS rest and are referred froim metromaps referrers\n",
    "    '''\n",
    "    return dataframe[\n",
    "    (dataframe['cs(Referer)'].str.contains('metromap')) &\n",
    "    (dataframe['cs-uri-stem'].str.contains('/services/')) &\n",
    "    (~dataframe['cs-uri-stem'].str.contains('/ArcGIS/')) &\n",
    "    (~dataframe['cs-uri-stem'].str.contains('/arcgis/'))\n",
    "    ]\n",
    "\n",
    "def parse_uri_stem(row):\n",
    "    '''\n",
    "    used in append_uri_path to break apart URI stem into a list, given a series with appropriate stem field'''\n",
    "    path = row['cs-uri-stem']\n",
    "    if pd.isnull(path):\n",
    "        return [None, None, None, None]\n",
    "    else:\n",
    "        split_values = path.split('/')\n",
    "        if len(split_values) >= 5:\n",
    "            return split_values[:5]\n",
    "        else:\n",
    "            split_values += [None] * (5 - len(split_values))\n",
    "            return split_values\n",
    "        \n",
    "def append_uri_path(dataframe):\n",
    "    '''\n",
    "    appends first 5 elements of a uri stem as separate fields to the end of a dataframe, for each row\n",
    "    '''\n",
    "\n",
    "    # Apply parse_uri_stem function to 'cs-uri-stem' column and store the results\n",
    "    split_values = dataframe.apply(parse_uri_stem, axis=1)\n",
    "\n",
    "    # Create new columns for the first five values in the split_values DataFrame\n",
    "    dataframe['1st_value'] = split_values.apply(lambda x: x[0])\n",
    "    dataframe['2nd_value'] = split_values.apply(lambda x: x[1])\n",
    "    dataframe['3rd_value'] = split_values.apply(lambda x: x[2])\n",
    "    dataframe['4th_value'] = split_values.apply(lambda x: x[3])\n",
    "    dataframe['5th_value'] = split_values.apply(lambda x: x[4])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def filter_metromaps_rlis(dataframe):\n",
    "    return dataframe[\n",
    "    (dataframe['cs-uri-stem'].str.contains('rlisapi')) & (dataframe['cs(Referer)'].str.contains('metromap'))\n",
    "    ]\n",
    "\n",
    "def user_agent_statistics(dataframe:pd.DataFrame, use_log:bool=True):\n",
    "    \"\"\"\n",
    "    looks at the distribution and characteristics of User Agent info,\n",
    "    and gives z-score of most prolific IP (base-2 logarithm-transformed if use_log)\n",
    "    \"\"\"\n",
    "    # Calculate the request count per IP address (log-transformed)\n",
    "    user_agent_counts = dataframe['cs(User-Agent)'].value_counts()\n",
    "    # print('user agent counts\\n', user_agent_counts)\n",
    "    if use_log:\n",
    "        user_agent_counts = np.log2(\n",
    "            user_agent_counts\n",
    "        )\n",
    "        \n",
    "\n",
    "    # calculate the average number of requests associated with a distinct user agent\n",
    "    mean_requests = user_agent_counts.mean()\n",
    "\n",
    "    # get dispersion of that statistic via stdev\n",
    "    std_requests = user_agent_counts.std()\n",
    "\n",
    "    # Identify the most common user agent (TODO: generalize by browser or ???)\n",
    "    most_prolific_ua = user_agent_counts.idxmax()\n",
    "\n",
    "    # Calculate the z-score for the most common user agent\n",
    "    z_score = (user_agent_counts[most_prolific_ua] - mean_requests) / std_requests\n",
    "\n",
    "    # check for suspish user agent strings\n",
    "    top10 = []\n",
    "    for agent in list(user_agent_counts.head(10).index):\n",
    "        top10.append(parse_user_agent(agent))\n",
    "\n",
    "    return {\n",
    "        # average number of requests associated with a distinct user agent\n",
    "        'mean_request_count': mean_requests,\n",
    "\n",
    "        # dispersion of that statistic via stdev\n",
    "        'stdev_request_count': std_requests,\n",
    "\n",
    "        # top 10 user agents\n",
    "        'top_10': top10,\n",
    "        'n_unique': dataframe['c-ip'].nunique(),\n",
    "        'top': {\n",
    "            'user_agent': most_prolific_ua,\n",
    "            'request_count': user_agent_counts.max(),\n",
    "            'z_score': z_score\n",
    "        },\n",
    "        'used_log': use_log\n",
    "    }\n",
    "\n",
    "# function to append browser to each request\n",
    "def extract_browser(user_agent):\n",
    "    parsed_agent = parse(user_agent)\n",
    "    return parsed_agent.browser.family\n",
    "\n",
    "\n",
    "# function to append os to each request\n",
    "def extract_os(user_agent):\n",
    "    parsed_agent = parse(user_agent)\n",
    "    return parsed_agent.os.family\n",
    "\n",
    "\n",
    "def check_svc_layer(dataframe):\n",
    "\n",
    "    svcs = filter_services(dataframe)\n",
    "    svcs_parsed = append_uri_path(svcs)\n",
    "\n",
    "    return {\n",
    "        '3rd value counts': svcs_parsed['3rd_value'].value_counts(),\n",
    "        '4th value counts': svcs_parsed['4th_value'].value_counts()\n",
    "    }\n",
    "\n",
    "def ua_stats_simple(dataframe:pd.DataFrame, n_classes:int=10):\n",
    "    \"\"\"\n",
    "    this function takes a log file dataframe,\n",
    "    and produces a dict containing the coutns of the n top browsers and oss,\n",
    "    according to the n_classes parameter. It also gives a count/proportion\n",
    "    of bots and mobile useres.\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(dataframe)\n",
    "\n",
    "    # add some user-agent fields (log, os) to each record in the dataframe\n",
    "    dataframe['browser'] = dataframe['cs(User-Agent)'].apply(extract_browser)\n",
    "    dataframe['os'] = dataframe['cs(User-Agent)'].apply(extract_os)\n",
    "\n",
    "    n_bots = len(dataframe[\n",
    "        dataframe['browser'].str.contains('bot') |\n",
    "        dataframe['browser'].str.contains('Bot') |\n",
    "        dataframe['browser'].str.contains('Peeker') |\n",
    "        dataframe['browser'].str.contains('peeker') |\n",
    "        dataframe['browser'].str.contains('Crawler') |\n",
    "        dataframe['browser'].str.contains('crawler') |\n",
    "        dataframe['browser'].str.contains('Spider') |\n",
    "        dataframe['browser'].str.contains('spider')\n",
    "        ])\n",
    "    \n",
    "    # NOTE: mobile # is inferred from browser, not OS.\n",
    "    n_mobile = len(dataframe[\n",
    "        dataframe['browser'].str.contains('mobile') |\n",
    "        dataframe['browser'].str.contains('Mobile') |\n",
    "        dataframe['browser'].str.contains('Snapchat') |\n",
    "        dataframe['browser'].str.contains('snapchat') |\n",
    "        dataframe['browser'].str.contains('Instagram') |\n",
    "        dataframe['browser'].str.contains('instagram')\n",
    "        ])\n",
    "    \n",
    "    top_browsers = dataframe.value_counts(['browser']).head(n_classes)\n",
    "    top_oss = dataframe.value_counts(['os']).head(n_classes)\n",
    "\n",
    "    # add # of browser/oss not represented by top n browser/os types to the Other category\n",
    "    if 'Other' in top_browsers:\n",
    "        top_browsers['Other'] += len(dataframe) - sum(top_browsers)\n",
    "    else:\n",
    "        print('Other not present in top rbwoser')\n",
    "        top_browsers['Other'] = len(dataframe) - sum(top_browsers)\n",
    "\n",
    "    if 'Other' in top_oss:\n",
    "        top_oss['Other'] += len(dataframe) - sum(top_oss)\n",
    "    else:\n",
    "        print('Other not present in top os')\n",
    "        top_oss['Other'] = len(dataframe) - sum(top_oss)\n",
    "    return {\n",
    "        'top_browsers': top_browsers,\n",
    "        'top_oss': top_oss,\n",
    "        'bot_info': {\n",
    "            'count': n_bots,\n",
    "            'prop': n_bots/total\n",
    "        },\n",
    "        'mobile_info': {\n",
    "            'count': n_mobile,\n",
    "            'prop': n_mobile/total\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: view and visualize request statistics by IP in a log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load log file for a given day\n",
    "oct4 = log_to_df(os.path.join(logfile_location, '202210/u_ex221004.log'))\n",
    "\n",
    "# get and view IP statistics (logarithm-transformed)\n",
    "stats = ip_request_statistics(oct4)\n",
    "print(\n",
    "    pprint.pformat(stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to show instances of the RLIS api  being targeted by metromaps referrers\n",
    "oc4_mm = oct4[\n",
    "    (oct4['cs-uri-stem'].str.contains('rlisapi')) & (oct4['cs(Referer)'].str.contains('metromap'))\n",
    "    ]\n",
    "oc4_mm['cs-uri-stem']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a a depiction of how prominent the most popular IP address is\n",
    "# visualize the distribution of requests per IP, using numpy and matplotlib\n",
    "x_axis = np.arange(\n",
    "    0,\n",
    "    stats['top']['request_count']\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    x_axis,\n",
    "    norm.pdf(x_axis, stats['mean_request_count'],\n",
    "             stats['stdev_request_count'])\n",
    ")\n",
    "\n",
    "plt.title('log-transformed distribution of requests per IP address\\n(most prolific IP request count in red)')\n",
    "\n",
    "# add most prolific IP as a red line\n",
    "plt.axvline(x=stats['top']['request_count'], color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the distribution to look at z-score\n",
    "x_axis = np.arange(-3, 3, 0.001)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, 1))\n",
    "plt.title('z-score of highest request count (red)')\n",
    "\n",
    "# adding z-score, in this case to a similar effect\n",
    "plt.axvline(x=stats['top']['z_score'], color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: view geographic distribution of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode a subset of requests in the IIS log dataframe\n",
    "# (NOTE: current geoip2 plan only allows 1000 request per day)\n",
    "oct4_geocoded = geocode_ip(oct4, sample_size=10)\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "# fyi this dataset was deprecated 2023\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "world.plot(ax=ax, color='#ffffff', edgecolor='#6a6a6a', linewidth=.5)\n",
    "oct4_geocoded.plot(ax=ax, color='red', markersize=1)\n",
    "_=ax.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: get average number of hits for each unique URI prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read through csv of common uri-stem prefixes, \n",
    "# get the average number of hits for each prefix\n",
    "folder_path = r'd:/metro/scripts/IISW3C-parse-tools/ua-by-uri/'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and create dataframes\n",
    "for filename in os.listdir(folder_path):\n",
    "    print(filename)\n",
    "    if filename.endswith('.CSV'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path, names=['Hits', 'Prefix'], skiprows=1)\n",
    "        dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes).groupby('Prefix').mean().reset_index()\n",
    "\n",
    "final = merged_df[merged_df['Hits'] > 10]\n",
    "\n",
    "# this targets '/services/*', excluding arcgis-hosted stuff\n",
    "services = merged_df[\n",
    "    (merged_df['Prefix'].str.contains('/services/')) &\n",
    "    ~(merged_df['Prefix'].str.contains('ArcGIS')) &\n",
    "    ~(merged_df['Prefix'].str.contains('arcgis'))\n",
    "    ]\n",
    "\n",
    "out = services.sort_values('Hits', ascending=False).reset_index()\n",
    "\n",
    "out.to_csv('avg_hits_per_2prefix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view most common browser for major URI prefixes\n",
    "log_data = log_to_df(r'd:/metro/logs/202210/u_ex221004.log')\n",
    "log_data = log_data[log_data['cs(Referer)'].str.contains('metromap')]\n",
    "# list of 2-folder URI prefixes\n",
    "prefixes = ['/rlisapi2/getsuggestion', '/arcgis/rest', '/metromap/img', '/rlisapi2/queryaddress', '/rlisapi2/js', '/assets/img', '/assets/fonts', '/metromap/js', '/services/taxlots', '/metromap/fonts', '/metromap/css', '/services/2ftcontours', '/rlisapi2/querypoint', '/services/citybounds', '/services/land', '/services/metrobounds', '/services/slope25', '/services/wetlands', '/services/riparianhabitat', '/services/floodplain', '/services/ugb', '/metromap/printserver.ashx', '/services/orca_parksandnaturalareas', '/services/slope10', '/services/sewerdistricts', '/services/uplandhabitat', '/services/reserves', '/authentication/js', '/services/haulers', '/metromap/print', '/services/boundary', '/services/vacantlands2013',\n",
    "            '/services/neighborhoods', '/services/councildistrict', '/services/trailsexisting', '/services/firedistricts', '/services/schooldistricts', '/services/waterdistrict', '/services/sectionlines', '/rlisapi2/searchtaxlotid', '/favicon.ico', '/services/zoning', '/services/orca_schools', '/services/transitdistrict', '/services/voterprecincts', '/services/lrt', '/services/orca_cemeteries', '/services/multifamily', '/services/orca_hoas', '/services/parkdistricts', '/services/orca_other', '/services/censustracts', '/metromap/', '/metromap/help.htm', '/services/orca_golfcourses', '/authentication/', '/metromap/embed', '/metromap/feedback.ashx', '/rlisapi2/searchtaxlotowner', '/historic_photo_viewer']\n",
    "\n",
    "for p in prefixes:\n",
    "    df = log_data[log_data['cs-uri-stem'].str.startswith(p)]\n",
    "    if len(df) > 0:\n",
    "        print(f'\\n\\n___{p} USER AGENT INFO___\\n')\n",
    "        pprint.pprint(user_agent_statistics(df))\n",
    "        print('____________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: view layers at /services/ referred by metromap, ranked by usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_referred_gislayers(df):\n",
    "    svcs = df[\n",
    "    (df['cs-uri-stem'].str.startswith('/services/')) &\n",
    "    (df['cs(Referer)'].str.contains('metromap'))\n",
    "    ]\n",
    "    svcs_no_arc = svcs[\n",
    "    ~(svcs['cs(Referer)'].str.contains('ArcGIS')) &\n",
    "    ~(svcs['cs(Referer)'].str.contains('arcgis'))\n",
    "    ]\n",
    "    svcs_parsed = append_uri_path(svcs)\n",
    "    svcs_no_arc_parsed = append_uri_path(svcs_no_arc)\n",
    "    print('incl. arcgis referrers:\\n', svcs_parsed.value_counts(['3rd_value']), '\\n\\n')\n",
    "    print('not incl. arcgis referrers:\\n', svcs_no_arc_parsed.value_counts(['3rd_value']), '\\n\\n')\n",
    "\n",
    "batch_stats(mm_referred_gislayers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: get and export user agent statistics to csv for visualization in Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a loop that gets user agent data for all time\n",
    "# takes 20 mins\n",
    "ua_dicts = []\n",
    "folders = [\n",
    "    os.path.join(logfile_location, '202210'),\n",
    "    os.path.join(logfile_location, '202211'),\n",
    "    os.path.join(logfile_location, '202212'),\n",
    "    os.path.join(logfile_location, '202301'),\n",
    "    os.path.join(logfile_location, '202302'),\n",
    "    os.path.join(logfile_location, '202303'),\n",
    "    os.path.join(logfile_location, '202304'),\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    logf = logs_to_df(folder)\n",
    "\n",
    "    # filter to target actual user visits to the metromap site \n",
    "    actual_visits = logf[logf['cs-uri-stem'].str.contains('/metromap/js/script.min.js')]\n",
    "    logf = None\n",
    "\n",
    "    # get stats and append to list\n",
    "    ua_dicts.append(ua_stats_simple(actual_visits))\n",
    "    actual_visits = None\n",
    "    \n",
    "\n",
    "ua_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ua_dicts into a csv representing all-time UA stats\n",
    "# Collapse the list of dictionaries into a single DataFrame\n",
    "# Extract operating system counts and export to a CSV file\n",
    "os_counts = pd.concat([data['top_oss'] for data in ua_dicts]).groupby(level=0).sum()\n",
    "os_counts.to_csv('operating_system_counts.csv')\n",
    "\n",
    "# Extract browser counts and export to a CSV file\n",
    "browser_counts = pd.concat([data['top_browsers'] for data in ua_dicts]).groupby(level=0).sum()\n",
    "browser_counts.to_csv('browser_counts.csv')\n",
    "\n",
    "# Extract bot information and export to a CSV file\n",
    "bot_info = pd.DataFrame([{'Bots Count': data['bot_info']['count'], 'Proportion': data['bot_info']['prop']} for data in ua_dicts])\n",
    "bot_info.to_csv('bot_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: delete weekends from a csv of daily hitcounts\n",
    "data for this example is extracted from the following logparser query and stored as csv in folders called `internal_counts` and `total_counts`. The CSVs are named by mo th, with October 2022 as 2210, January 2023 as 2301, etc.\n",
    "\n",
    "```\n",
    "/*  New Query  */\n",
    "SELECT TO_LOCALTIME(QUANTIZE(TO_TIMESTAMP(date, time), 3600)) AS [Hour],\n",
    "         COUNT(*) AS [Requests]\n",
    "    FROM '[LOGFILEPATH]'\n",
    "WHERE (cs-uri-stem = '/metromap/js/script.min.js') and (\n",
    "c-ip LIKE '10.%'\n",
    "OR c-ip LIKE '172.16.%'\n",
    "OR c-ip LIKE '172.17.%'\n",
    "OR c-ip LIKE '172.18.%'\n",
    "OR c-ip LIKE '172.19.%'\n",
    "OR c-ip LIKE '172.2%.'\n",
    "OR c-ip LIKE '172.30.%'\n",
    "OR c-ip LIKE '172.31.%'\n",
    "OR c-ip LIKE '192.168.%'\n",
    ")\n",
    "\n",
    "GROUP BY Hour\n",
    "ORDER BY Hour\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def is_weekday(csv_row:str, col_num:int, date_format:str='%m/%d/%Y %I:%M:%S %p', delim='\\t'):\n",
    "    \"\"\"\n",
    "    checks the timestamp a string (delimiter speciifed by delim param),\n",
    "    located via index speficied in the col_num parameter,\n",
    "    and reports wether or not it's a weekday\n",
    "    \"\"\"\n",
    "    # get timestamp\n",
    "    ts = datetime.strptime(\n",
    "        csv_row.split(delim)[col_num],\n",
    "        date_format\n",
    "    )\n",
    "\n",
    "    # monday = 1, friday = 4\n",
    "    if ts.weekday() < 5:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average hits for non-weekdays\n",
    "def avg_hits_weekdays(location:str):\n",
    "    weekday_counts = {}\n",
    "    with open(location, 'r') as counts:\n",
    "        for line in counts:\n",
    "            if line.split('\\t')[1].startswith('Hour'):\n",
    "                pass\n",
    "\n",
    "            elif is_weekday(line, 1):\n",
    "                # it will be formatted like \n",
    "                # 2/8/2023 1:00:00 PM\n",
    "                vals = line.split('\\t')\n",
    "                time = vals[1].split(' ')[1:3]\n",
    "                hour = time[0] + ' ' + time[1]\n",
    "                if hour not in weekday_counts.keys():\n",
    "                    weekday_counts[hour] = []\n",
    "                weekday_counts[hour].append(int(vals[2].strip('\\n')))\n",
    "    print(f'daily hits for {location}:\\n\\t', weekday_counts, '\\n')\n",
    "    return weekday_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = [\n",
    "    '12:00:00 AM',\n",
    "    '1:00:00 AM',\n",
    "    '2:00:00 AM',\n",
    "    '3:00:00 AM',\n",
    "    '4:00:00 AM',\n",
    "    '5:00:00 AM',\n",
    "    '6:00:00 AM',\n",
    "    '7:00:00 AM',\n",
    "    '8:00:00 AM',\n",
    "    '9:00:00 AM',\n",
    "    '10:00:00 AM',\n",
    "    '11:00:00 AM',\n",
    "    '12:00:00 PM',\n",
    "    '1:00:00 PM',\n",
    "    '2:00:00 PM',\n",
    "    '3:00:00 PM',\n",
    "    '4:00:00 PM',\n",
    "    '5:00:00 PM',\n",
    "    '6:00:00 PM',\n",
    "    '7:00:00 PM',\n",
    "    '8:00:00 PM',\n",
    "    '9:00:00 PM',\n",
    "    '10:00:00 PM',\n",
    "    '11:00:00 PM'\n",
    "]\n",
    "month_dfs = {}\n",
    "\n",
    "# for each month,\n",
    "for month in os.listdir('./internal_counts'):\n",
    "    hit_data = {}\n",
    "    hit_data['internal'] = avg_hits_weekdays(f'./internal_counts/{month}')\n",
    "    hit_data['total'] = avg_hits_weekdays(f'./total_counts/{month}')\n",
    "\n",
    "    # get a dataframe whose index is hour (from 0000 to 2400)\n",
    "    # with columns for internal and total traffic, at hourly intervals\n",
    "    hourly = pd.DataFrame()\n",
    "    hourly.index = hours\n",
    "    \n",
    "    # populate the dataframe with data from the weekdays function\n",
    "    for hour in hours:\n",
    "        for i in ['internal', 'total']:\n",
    "            try:\n",
    "                hourly.loc[hour, f'{i}'] = round(np.sum(hit_data[i][hour]), 2)\n",
    "                # hourly.loc[hour, f'{i} mean'] = round(np.mean(hit_data[i][hour]), 2)\n",
    "\n",
    "            except KeyError:\n",
    "                hourly.loc[hour, f'{i}'] = np.nan\n",
    "                # hourly.loc[hour, f'{i} mean'] = np.nan\n",
    "\n",
    "    # add a final row representing to the total average number of internal/total hits in that 12 hour period.\n",
    "    month_dfs[month.strip('.csv')] = hourly\n",
    "    hourly = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view findings for April 2023\n",
    "month_dfs['2304']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
